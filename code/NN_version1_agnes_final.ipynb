{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3f59c7",
   "metadata": {},
   "source": [
    "# Neural Network using tensorflow\n",
    "for this model whe used some information from \"https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30799972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import tensorflow.keras as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import keras_tuner as kt\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6083f3f",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03015da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "clean_test_df = pd.read_csv('data/clean_test_data.csv')\n",
    "clean_train_df = pd.read_csv('data/clean_train_data.csv')\n",
    "top10_test_df = pd.read_csv('data/top10_test_data.csv')\n",
    "top10_train_df = pd.read_csv('data/top10_train_data.csv')\n",
    "\n",
    "display(test_df)\n",
    "display(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(df_train, df_test):\n",
    "    \n",
    "    # split data into input X and target Y\n",
    "    target_train = train_df['SalePrice']\n",
    "    target_test = test_df['SalePrice']\n",
    "\n",
    "    input_train = train_df.drop('SalePrice', axis=1)\n",
    "    input_test = test_df.drop('SalePrice', axis=1)\n",
    "    \n",
    "    # convert the pandas dataframes to numpy ndarrays\n",
    "    X_train = input_train.to_numpy()\n",
    "    X_test = input_test.to_numpy()\n",
    "    y_train = target_train.to_numpy()\n",
    "    y_test = target_test.to_numpy()\n",
    "\n",
    "    # find number of features\n",
    "    n_features = input_train.shape[1]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149af8d",
   "metadata": {},
   "source": [
    "## Build the model\n",
    "The code that is written computes its own neural network models and picks the best out of it. We continue with that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    This function computes a/the best neural network for the given data. \n",
    "    It makes a model by tuning the layers and parameters of the layers for \n",
    "    the amount of trials given in the tuner variable.\n",
    "    \n",
    "    source: \"https://keras.io/guides/keras_tuner/getting_started/\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # metrics for the layers\n",
    "    m1 = tf.metrics.RootMeanSquaredError()\n",
    "    m2 = 'mean_absolute_percentage_error'\n",
    "    \n",
    "    # compute a model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # tune number of layers\n",
    "    for i in range(hp.Int(\"numlayers\", 1, 4)):\n",
    "        model.add(\n",
    "            Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units{i}\", min_value=16, max_value=256, step=16),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"leaky_relu\", \"elu\", \"tanh\"])),\n",
    "            )\n",
    "        \n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(Dropout(rate=0.25))\n",
    "        \n",
    "    # check if batch normalization is benneficial\n",
    "    if hp.Boolean(\"bn_after_act\"):\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    model.compile(optimizer='Adam', loss=tf.metrics.mean_squared_error, metrics=[m1, m2])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=100,\n",
    "    executions_per_trial=2,\n",
    "    #overwrite=True,\n",
    "    #directory=\"data\",\n",
    "    #project_name=\"Milestone_3\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(X_test, y_test):\n",
    "\n",
    "    loss_df = pd.DataFrame(best_model.history.history)\n",
    "\n",
    "    loss_df.plot(figsize=(12,8), )\n",
    "    plt.title(\"Model information\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.yscale('log')\n",
    "    plt.show()\n",
    "\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    var_score = metrics.explained_variance_score(y_test,y_pred)\n",
    "\n",
    "    # compute the accuracy of the model \n",
    "    print('Variance score:', var_score)\n",
    "    print('\\nRMSE:',loss_df['root_mean_squared_error'].tail(1))\n",
    "    print('\\nval RMSE:',loss_df['val_root_mean_squared_error'].tail(1))\n",
    "    print('\\nTrain Loss:',loss_df['loss'].tail(1))\n",
    "    print('\\nTest Loss:',loss_df['val_loss'].tail(1))\n",
    "    print('\\nMAPE:',loss_df['mean_absolute_percentage_error'].tail(1))\n",
    "    print('\\nval MAPE:',loss_df['val_mean_absolute_percentage_error'].tail(1))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ecc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dataframes to numpy arrays\n",
    "clean_X_train, clean_X_test, clean_y_train, clean_y_test, n_features = prepare_data(clean_df_train, clean_df_test)\n",
    "top10_X_train, top10_X_test, top10_y_train, top10_y_test, n_features = prepare_data(top10_df_train, top10_df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb84156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the models with all data and get best model\n",
    "tuner.search(clean_X_train, clean_y_train, epochs=5, validation_data=(clean_X_test, clean_y_test))\n",
    "best_model = tuner.get_best_models()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e03294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the best model on all data\n",
    "best_model.fit(clean_X_train, clean_y_train,\n",
    "          batch_size=32, epochs=200,\n",
    "          validation_data=(clean_X_test, clean_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8203b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuner results:\n",
    "best_model.summary()\n",
    "tuner.search_space_summary()\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(clean_X_test, clean_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the best model on top features\n",
    "best_model.fit(top10_X_train_features, top10_y_train_features,\n",
    "          batch_size=32, epochs=200,\n",
    "          validation_data=(top10_X_test, top10_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a526e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(top10_X_test, top10_y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6bb825",
   "metadata": {},
   "source": [
    "## Visualizing the results\n",
    "Now that the model is actually build, we can visualize the outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import visualkeras\n",
    "#from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydot\n",
    "\n",
    "tf.utils.model_to_dot(\n",
    "    best_model,\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    subgraph=False,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    ")\n",
    "\n",
    "tf.utils.plot_model(\n",
    "    best_model,\n",
    "    to_file=\"model.png\",\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir=\"TB\",\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

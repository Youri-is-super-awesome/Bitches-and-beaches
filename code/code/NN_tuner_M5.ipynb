{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac3f59c7",
   "metadata": {},
   "source": [
    "# Neural Network using tensorflow\n",
    "for this model whe used some information from \"https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30799972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow.keras as tf\n",
    "import keras_tuner as kt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, BatchNormalization, Dropout, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import InputLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ee6168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set on True if keras tuner needs to search for new best model\n",
    "search_best_model = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6083f3f",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03015da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean is with all features and preprocess is filtered on features\n",
    "clean_test_df = pd.read_csv('data/clean_test_data.csv')\n",
    "clean_train_df = pd.read_csv('data/clean_train_data.csv')\n",
    "preprocess_test_df = pd.read_csv('data/preprocess_test_data.csv')\n",
    "preprocess_train_df = pd.read_csv('data/preprocess_train_data.csv')\n",
    "\n",
    "# load best features\n",
    "best_features_df = pd.read_csv('data/bestfeatures_data.csv')\n",
    "\n",
    "best_features = []\n",
    "\n",
    "for feature in best_features_df:\n",
    "    best_features.append(feature) \n",
    "\n",
    "# create new dataframes for test and train with only best features\n",
    "best_features_test_df = preprocess_test_df.loc[:, best_features]\n",
    "best_features_train_df = preprocess_train_df.loc[:, best_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94df2ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(train_df, test_df):\n",
    "    \n",
    "    # split data into input X and target Y\n",
    "    target_train = train_df['SalePrice']\n",
    "    target_test = test_df['SalePrice']\n",
    "\n",
    "    input_train = train_df.drop('SalePrice', axis=1)\n",
    "    input_test = test_df.drop('SalePrice', axis=1)\n",
    "    \n",
    "    # convert the pandas dataframes to numpy ndarrays\n",
    "    X_train = input_train.to_numpy()\n",
    "    X_test = input_test.to_numpy()\n",
    "    y_train = target_train.to_numpy()\n",
    "    y_test = target_test.to_numpy()\n",
    "\n",
    "    # find number of features\n",
    "    n_features = input_train.shape[1]\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f149af8d",
   "metadata": {},
   "source": [
    "## Building the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7806b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(hp):\n",
    "    \"\"\"\n",
    "    This function computes a/the best neural network for the given data. \n",
    "    It makes a model by tuning the layers and parameters of the layers for \n",
    "    the amount of trials given in the tuner variable.\n",
    "    \n",
    "    source: \"https://keras.io/guides/keras_tuner/getting_started/\"\n",
    "    \"\"\"\n",
    "    \n",
    "    # metrics for the layers\n",
    "    m1 = tf.metrics.RootMeanSquaredError()\n",
    "    m2 = 'mean_absolute_percentage_error'\n",
    "    \n",
    "    # compute a model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # tune number of layers\n",
    "    for i in range(hp.Int(\"numlayers\", 1, 4)):\n",
    "        model.add(\n",
    "            Dense(\n",
    "                # Tune number of units separately.\n",
    "                units=hp.Int(f\"units{i}\", min_value=16, max_value=256, step=16),\n",
    "                activation=hp.Choice(\"activation\", [\"relu\", \"leaky_relu\", \"elu\", \"tanh\"])),\n",
    "            )\n",
    "        \n",
    "    if hp.Boolean(\"dropout\"):\n",
    "        model.add(Dropout(rate=0.25))\n",
    "        \n",
    "    # check if batch normalization is benneficial\n",
    "    if hp.Boolean(\"bn_after_act\"):\n",
    "        model.add(BatchNormalization())\n",
    "        \n",
    "    # output layer\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='Adam', loss=tf.metrics.mean_squared_error, metrics=[m1, m2])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1527db35",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = kt.RandomSearch(\n",
    "    hypermodel=build_model,\n",
    "    objective=\"val_loss\",\n",
    "    max_trials=300,\n",
    "    executions_per_trial=3,\n",
    "    overwrite=True,\n",
    "    directory=\"keras_tuner\",\n",
    "    project_name=\"tuner_trials\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524a060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_NN(n_features):\n",
    "    \"\"\"This function recreates best model for the neural network of Milestone 5\"\"\"\n",
    "    \n",
    "    # metrics for the layers\n",
    "    m1 = tf.metrics.RootMeanSquaredError()\n",
    "    m2 = 'mean_absolute_percentage_error'\n",
    "    \n",
    "    # build model and add layers\n",
    "    model = tf.Sequential([\n",
    "        Dense(160),\n",
    "        Dense(224, activation='tanh'),\n",
    "        Dense(112, activation='tanh'),\n",
    "        Dense(240, activation='tanh'),\n",
    "        Dense(1, input_shape=(n_features,)),\n",
    "        ])\n",
    "    \n",
    "    # compile model\n",
    "    model.compile(optimizer='Adam', loss=tf.metrics.mean_squared_error, metrics=[m1, m2])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ac281f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results(X_test, y_test, model):\n",
    "    \n",
    "    loss_df = pd.DataFrame(model.history.history)\n",
    "    loss_df['root_mean_squared_error'].plot(figsize=(12,8))\n",
    "    loss_df['val_root_mean_squared_error'].plot(figsize=(12,8))\n",
    "    \n",
    "    plt.title(\"Model information\")\n",
    "    plt.xlabel(\"epochs\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    var_score = metrics.explained_variance_score(y_test,y_pred)\n",
    "\n",
    "    # compute the accuracy of the model \n",
    "    print('Variance score:', var_score)\n",
    "    print('\\nRMSE:',loss_df['root_mean_squared_error'].tail(1))\n",
    "    print('\\nval RMSE:',loss_df['val_root_mean_squared_error'].tail(1))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7ecc08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform dataframes to numpy arrays\n",
    "clean_X_train, clean_X_test, clean_y_train, clean_y_test, clean_n_features = prepare_data(clean_train_df, clean_test_df)\n",
    "preprocess_X_train, preprocess_X_test, preprocess_y_train, preprocess_y_test, preprocess_n_features = prepare_data(preprocess_train_df, preprocess_test_df)\n",
    "best_features_X_train, best_features_X_test, best_features_y_train, best_features_y_test, best_features_n_features = prepare_data(best_features_train_df, best_features_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b280332d",
   "metadata": {},
   "source": [
    "#### Build model for clean data\n",
    "Model is fit and run with complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb84156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for new best model\n",
    "if search_best_model == True:\n",
    "    \n",
    "    # train the models with all data and get best model\n",
    "    tuner.search(clean_X_train, clean_y_train, epochs=10, validation_data=(clean_X_test, clean_y_test))\n",
    "    best_model = tuner.get_best_models()[0]\n",
    "\n",
    "# reuse last best model\n",
    "else:\n",
    "    best_model = build_NN(clean_n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e03294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the best model on all data\n",
    "best_model.fit(clean_X_train, clean_y_train,\n",
    "          batch_size=32, epochs=300,\n",
    "          validation_data=(clean_X_test, clean_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bfa1601",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.search_space_summary()\n",
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8203b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110b55d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(clean_X_test, clean_y_test, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93d35e6",
   "metadata": {},
   "source": [
    "#### Build model for preprocessed data\n",
    "Model is build with preprocessed data that is filtered on 'weak' features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3fb2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = tf.metrics.RootMeanSquaredError()\n",
    "m2 = 'mean_absolute_percentage_error'\n",
    "\n",
    "# create model with the same configuration as best model\n",
    "model = tf.Sequential()\n",
    "\n",
    "# set correct input layer nodes\n",
    "nodes_first_layer = best_model.layers[0].output_shape[1]\n",
    "\n",
    "# but change the input dimensions of the input layer to that of preprocessed data\n",
    "model.add(Dense(nodes_first_layer, input_shape=(preprocess_n_features,)))\n",
    "\n",
    "# add all the other layers of best model\n",
    "for layer in range(1, len(best_model.layers)):\n",
    "    model.add(best_model.layers[layer])\n",
    "\n",
    "model.compile(optimizer='Adam', loss=tf.metrics.mean_squared_error, metrics=[m1, m2])\n",
    "\n",
    "# fit model with preprocessed data (only including a selection of features)\n",
    "model.fit(preprocess_X_train, preprocess_y_train,\n",
    "          batch_size=32, epochs=300,\n",
    "          validation_data=(preprocess_X_test, preprocess_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a526e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d86b843",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(preprocess_X_test, preprocess_y_test, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074c5a83",
   "metadata": {},
   "source": [
    "#### Build model for best features data\n",
    "Model is build with best_features data that ony contains the top 10 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8874fb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model with the same configuration as best model\n",
    "model2 = tf.Sequential()\n",
    "\n",
    "# set correct input layer nodes\n",
    "nodes_first_layer = best_model.layers[0].output_shape[1]\n",
    "\n",
    "# but change the input dimensions of the input layer to that of preprocessed data\n",
    "model2.add(Dense(nodes_first_layer, input_shape=(best_features_n_features,)))\n",
    "\n",
    "# add all the other layers of best model\n",
    "for layer in range(1, len(best_model.layers)):\n",
    "    model2.add(best_model.layers[layer])\n",
    "\n",
    "model2.compile(optimizer='Adam', loss=tf.metrics.mean_squared_error, metrics=[m1, m2])\n",
    "\n",
    "# fit model with preprocessed data (only including a selection of features)\n",
    "model2.fit(best_features_X_train, best_features_y_train,\n",
    "          batch_size=32, epochs=300,\n",
    "          validation_data=(best_features_X_test, best_features_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848938b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7bf5605",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_results(best_features_X_test, best_features_y_test, model2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1cc2c7e",
   "metadata": {},
   "source": [
    "# Regression model: Predict Ames house prices\n",
    "\n",
    "Estimate the value of houses in Ames, Iowa. This notebook builds and compares a univariate and multivariate regression model. The first uses the feature that is most strongly correlated with sale price, the second uses the two most strongest correlated features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90cb838",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10e266d",
   "metadata": {},
   "source": [
    "### Importing and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96aba434",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the data\n",
    "data = pd.read_csv('data/AmesHousingNumData.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e92e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into input and target\n",
    "target = data['SalePrice']\n",
    "final_data = data.drop('SalePrice', axis=1)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfe0ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the pandas dataframes to numpy ndarrays\n",
    "X_np = final_data.to_numpy()\n",
    "y_np = target.to_numpy()\n",
    "\n",
    "# split the data into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, train_size=0.7, random_state=1265599650)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc16dfe",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec6b316",
   "metadata": {},
   "source": [
    "#### Visualizing a single feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bc3179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make column vector out of row vector\n",
    "y_train = y_train[..., None]\n",
    "\n",
    "# make column vector for one feature for testing\n",
    "X_single_feature = X_train[:, 1, None]\n",
    "\n",
    "# retrieve name of said feature\n",
    "feature_name = final_data.columns.values[1]\n",
    "\n",
    "# plot the datapoints\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(X_single_feature, y_train, 'r.')\n",
    "plt.title(f'Relation of {feature_name} and Sale Price')\n",
    "plt.xlabel(f'{feature_name}')\n",
    "plt.ylabel('Sale Price normalized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eb50ea",
   "metadata": {},
   "source": [
    "#### Adding  $x_0$to the data\n",
    "\n",
    "First an extra feature $x_0$ has to be added to the data. This is to make the computations easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7559705",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_x0(x):\n",
    "    '''\n",
    "    This function takes a column vector x, adds a column vector of ones in front of it, and returns the \n",
    "    combined matrix.\n",
    "    '''\n",
    "    # create new feature vector of ones in shape of number of samples * 1 column vector \n",
    "    x_zero = np.ones((len(x), 1), dtype = int)\n",
    "\n",
    "    # combine the new and old feature vectors into matrix\n",
    "    return np.hstack((x_zero, x))\n",
    "\n",
    "# create feature with x0\n",
    "X = add_x0(X_single_feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a45626",
   "metadata": {},
   "source": [
    "#### Defining the linear model\n",
    "\n",
    "\n",
    "First, we implement **univariate linear regression** with the feature selected above. The system of linear equations looks like this:\n",
    "\n",
    "$$x^1_0 \\theta_0 + x^1_1 \\theta_1 = h_{\\theta}(x^1)$$\n",
    "$$x^2_0 \\theta_0 + x^2_1 \\theta_1 = h_{\\theta}(x^2)$$\n",
    "$$x^3_0 \\theta_0 + x^3_1 \\theta_1 = h_{\\theta}(x^3)$$ \n",
    "$$\\dots$$\n",
    "$$x^m_0 \\theta_0 + x^m_1 \\theta_1 = h_{\\theta}(x^m)$$\n",
    "\n",
    "\n",
    "x_0 is the column vector of ones that were added in front of the column vector of the selected feature. \n",
    "\n",
    "In terms of matrix multiplication:\n",
    "\n",
    "$$ \\left[\\begin{array}{cccc}\n",
    "x^1_0 & x^1_1 \\\\ \n",
    "x^2_0 & x^2_1 \\\\\n",
    "x^3_0 & x^3_1 \\\\ \n",
    "\\vdots & \\vdots \\\\\n",
    "x^m_0 & x^m_1 \\\\ \n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\end{array} \\right]\n",
    "= \\left[\\begin{array}{c} h_{\\theta}(x^1) \\\\ h_{\\theta}(x^2) \\\\\n",
    "h_{\\theta}(x^3) \\\\ \\vdots \\\\ h_{\\theta}(x^m) \\end{array} \\right]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3197ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_theta(X):\n",
    "    '''\n",
    "    Returns starting vector for theta with amount of features.\n",
    "    '''\n",
    "    return np.ones((X.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8854c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X, theta):\n",
    "    '''\n",
    "    This function takes a matrix X and a vector theta and returns the resulting column vector of hypothesis\n",
    "    values.\n",
    "    '''\n",
    "    return np.matmul(X, theta)\n",
    "\n",
    "theta = define_theta(X)\n",
    "\n",
    "# compute the linear model\n",
    "h = linear_model(X, theta)\n",
    "\n",
    "# plot the model results for just one of the features\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(X_single_feature, y_train, 'r.')\n",
    "plt.plot(X_single_feature, h)\n",
    "plt.title(f'Relation of {feature_name} and Sale Price')\n",
    "plt.xlabel(f'{feature_name}')\n",
    "plt.ylabel('Sale Price normalized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e960a688",
   "metadata": {},
   "source": [
    "### Optimizing theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e58b2e",
   "metadata": {},
   "source": [
    "#### Defining linear cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443ecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cost(theta, X, y):\n",
    "    '''\n",
    "    This function returns the cost using matrix multiplication. \n",
    "    '''\n",
    "    \n",
    "    # m is identical to the number of rows in X\n",
    "    m = X.shape[0]\n",
    "\n",
    "    # compute error vector \n",
    "    residual = linear_model(X, theta) - y\n",
    "    \n",
    "    # compute cost vector  \n",
    "    cost = np.matmul(residual.T, residual) / (2 * m)\n",
    "    \n",
    "    # convert the resulting 1 x 1 numpy matrix to an actual scalar \n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda544d0",
   "metadata": {},
   "source": [
    "#### Defining gradient vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b69ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_vector(theta, X, y):\n",
    "    '''\n",
    "    This functions returns a (n + 1) * 1 column vector of partial derivatives. \n",
    "    '''\n",
    "    \n",
    "    # m is identical to the number of rows in X\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # compute error vector\n",
    "    error = linear_model(X, theta) - y \n",
    "    \n",
    "    # compute matrix product of the error and X\n",
    "    gradient_vector = np.matmul(X.T, error) / m\n",
    "  \n",
    "    return gradient_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dacb235",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, thres=10**-6):\n",
    "    '''\n",
    "    This function returns a vector for theta for which the cost is the minimum. \n",
    "    '''\n",
    "    # calculate cost with current theta\n",
    "    previous_cost = linear_cost(theta, X, y)\n",
    "    \n",
    "    # set cost difference to amount bigger than the threshold and bigger than 0\n",
    "    cost_difference = thres + 1\n",
    "    \n",
    "    # loop indefinitely until algorithm converges\n",
    "    while cost_difference > thres: \n",
    "        \n",
    "        # compute and update theta parameters in a single operation\n",
    "        theta = theta - alpha * gradient_vector(theta, X, y)\n",
    "\n",
    "        # calculate cost vector with new theta\n",
    "        current_cost = linear_cost(theta, X, y)\n",
    "\n",
    "        cost_difference = abs(current_cost - previous_cost)\n",
    "\n",
    "        if current_cost > previous_cost:\n",
    "            print('The algorithm is diverging.')\n",
    "            return theta, previous_cost\n",
    "\n",
    "        previous_cost = current_cost\n",
    "    \n",
    "    return theta, previous_cost\n",
    "\n",
    "# find the theta vector that minimizes the cost function\n",
    "theta_hat, cost = gradient_descent(X, y_train, theta, 0.9)\n",
    "print(\"\\nFound the following values for theta:\")\n",
    "print(theta_hat)\n",
    "print(f'\\nThe cost is {cost}.')\n",
    "\n",
    "# plot the model results\n",
    "print(\"\\nResulting in the following model:\")\n",
    "h = linear_model(X, theta_hat)\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(X_single_feature, y_train, 'r.')\n",
    "plt.plot(X_single_feature, h)\n",
    "plt.title(f'Relation of {feature_name} and Sale Price')\n",
    "plt.xlabel(f'{feature_name}')\n",
    "plt.ylabel('Sale Price in logarithmic scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412aaeb",
   "metadata": {},
   "source": [
    "#### Visualizing multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0a0020",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# make matrix for all features (two for now)\n",
    "X_all_train = X_train[:, 0:]\n",
    "\n",
    "#retrieve name of second feature\n",
    "second_feature_name = final_data.columns.values[0]\n",
    "\n",
    "#create 3d plot\n",
    "ax = Axes3D(plt.figure(figsize=(12,9)))\n",
    "\n",
    "# retrieve datapoints and plot\n",
    "xs = X_all_train[:, 0]\n",
    "ys = X_all_train[:, 1]\n",
    "zs = y_train\n",
    "ax.scatter(xs, ys, zs)\n",
    "\n",
    "# set layout\n",
    "ax.set_xlabel(f'{second_feature_name}', fontsize=14)\n",
    "ax.set_ylabel(f'{feature_name}', fontsize=14)\n",
    "ax.set_zlabel('Sale Price in logarithmic scale', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d085dbc",
   "metadata": {},
   "source": [
    "#### Defining multivariate linear regression\n",
    "\n",
    "Our model is a **multivariate linear** regression model which will look like the following:\n",
    "\n",
    "$$x^1_0 \\theta_0 + x^1_1 \\theta_1 + \\dots + x^1_n \\theta_n = h_{\\theta}(x^1)$$\n",
    "$$x^2_0 \\theta_0 + x^2_1 \\theta_1 + \\dots + x^2_n \\theta_n = h_{\\theta}(x^2)$$\n",
    "$$x^2_0 \\theta_0 + x^2_1 \\theta_1 + \\dots + x^2_n \\theta_n = h_{\\theta}(x^2)$$\n",
    "$$\\dots$$\n",
    "$$x^m_0 \\theta_0 + x^m_1 \\theta_1 + \\dots + x^m_n \\theta_n = h_{\\theta}(x^m)$$\n",
    "\n",
    "\n",
    "In terms of matrix multiplication:\n",
    "\n",
    "$$ \\left[\\begin{array}{cccc}\n",
    "x_0^1 & x_1^1 & x_2^1 & \\cdots & x_n^1 \\\\ \n",
    "x_0^2 & x_1^2 & x_2^2 & \\cdots & x_n^2 \\\\ \n",
    "x_0^3 & x_1^3 & x_2^3 & \\cdots & x_n^3 \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_0^m & x_1^m & x_2^m & \\cdots & x_n^m \\\\ \n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\\\\n",
    "\\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{array} \\right]\n",
    "= \\left[\\begin{array}{c} h_{\\theta}(x^1) \\\\ h_{\\theta}(x^2) \\\\\n",
    "h_{\\theta}(x^3) \\\\ \\vdots \\\\ h_{\\theta}(x^m) \\end{array} \\right]$$\n",
    "\n",
    "With *n* being the number of features and *m* the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add x zero\n",
    "X_train_x0 = add_x0(X_all_train)\n",
    "\n",
    "theta = define_theta(X_train_x0)\n",
    "\n",
    "# compute the linear model\n",
    "h = linear_model(X_train_x0, theta)\n",
    "\n",
    "theta_hat, cost = gradient_descent(X_train_x0, y_train, theta, 0.05)\n",
    "print(\"\\nFound the following values for theta:\")\n",
    "print(theta_hat)\n",
    "print(f'\\nThe cost is {cost}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39439812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_learning_curves(X_train, y_train, X_test, y_test, theta, alpha, thres):\n",
    "    '''\n",
    "    Implements gradient descent, stores training and testing cost at every step, and plots learning curves.\n",
    "    '''\n",
    "    # initialize number of iterations\n",
    "    iteration = 0\n",
    "    iterations = []\n",
    "    \n",
    "    # create lists for training and testing cost to keep track at each step\n",
    "    current_costs_train = []\n",
    "    current_costs_test = []\n",
    "\n",
    "    # compute training cost with start theta\n",
    "    previous_cost = linear_cost(theta, X_train, y_train)\n",
    "\n",
    "    cost_diff = thres + 1\n",
    "\n",
    "    while cost_diff > thres:\n",
    "\n",
    "        iteration += 1\n",
    "        iterations.append(iteration)\n",
    "        \n",
    "        # compute new theta\n",
    "        theta = theta - alpha * gradient_vector(theta, X_train, y_train)\n",
    "        \n",
    "        # compute new training cost and append it to a list\n",
    "        current_cost_train = linear_cost(theta, X_train, y_train)\n",
    "        current_costs_train.append(current_cost_train)\n",
    "        \n",
    "        # compute new testing cost and append it to a list\n",
    "        current_cost_test = linear_cost(theta, X_test, y_test)\n",
    "        current_costs_test.append(current_cost_test)\n",
    "\n",
    "        # compare the old cost vs the new cost\n",
    "        cost_diff = previous_cost - current_cost_train\n",
    "\n",
    "        # check if the algorithm is diverging\n",
    "        if cost_diff < 0:\n",
    "            print('The algorithm is diverging.')\n",
    "\n",
    "        previous_cost = current_cost_train\n",
    "        \n",
    "    # plot learning curves\n",
    "    plt.plot(iterations, current_costs_train, 'b-', label='Training')\n",
    "    plt.plot(iterations, current_costs_test, 'r-', label='Testing')    \n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return theta, current_cost_train, current_cost_test\n",
    "\n",
    "# make matrix for all features \n",
    "X_all_test = X_test[:, 0:]\n",
    "\n",
    "# add x zero\n",
    "X_test_x0 = add_x0(X_all_test)\n",
    "\n",
    "# make column vector out of row vector\n",
    "y_test = y_test[..., None]\n",
    "\n",
    "# initialize theta to a zero vector of the correct shape\n",
    "theta = define_theta(X_train_x0)\n",
    "\n",
    "# find the theta vector that minimizes the cost functions\n",
    "plt.title('Learning curves of gradient descent with learning rate = 0.8, threshold = 10**-4\\n')\n",
    "theta_hat, cost_train, cost_test = gradient_descent_learning_curves(X_train_x0, y_train, X_test_x0, y_test, theta, 0.8, 10**-4)\n",
    "print(f'\\nThe training cost is {cost_train}. The testing cost is {cost_test}.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3aad04d",
   "metadata": {},
   "source": [
    "#### Prediction accuracy\n",
    "\n",
    "Taken from https://www.kaggle.com/kamalp0/ames-housing-regression-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587b59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "predictions = linear_model(X_test_x0, theta_hat)\n",
    "\n",
    "# calculate average error\n",
    "MSE = mean_squared_error(y_test, predictions)\n",
    "\n",
    "# calculate standard deviation\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "# calculate average price\n",
    "avg_price = np.mean(target)\n",
    "off_by = (RMSE)/ avg_price* 100\n",
    "\n",
    "print(f'The test voor MSE for our linear model was {MSE}')\n",
    "print(f'The test RMSE for our linear model {RMSE}')\n",
    "print(f'The percentage our model was off compared to the real house price was:{off_by}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562dc947",
   "metadata": {},
   "source": [
    "### Sklearn regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43fda682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a second linear regression model with the sklearn function\n",
    "linear_model2 = LinearRegression()\n",
    "\n",
    "linear_model2.fit(X_train, y_train)\n",
    "\n",
    "predict = linear_model2.predict(X_test)\n",
    "\n",
    "linear_test_mae = mean_absolute_error(y_test, predict)\n",
    "linear_test_rmse = np.sqrt(mean_squared_error(y_test, predict))\n",
    "\n",
    "print(f'The test MAE for our linear model {linear_test_mae}')\n",
    "print(f'The test RMSE for our linear model {linear_test_rmse}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3522f92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

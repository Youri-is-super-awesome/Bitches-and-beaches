{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57815593",
   "metadata": {},
   "source": [
    "# Regression model for determing the housing prices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3583cc",
   "metadata": {},
   "source": [
    "## importing and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c40a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e69a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data (only numerical)\n",
    "# data = pd.read_csv('data/AmesHousingNumData.csv')\n",
    "# data = pd.read_csv('data/AmesHousingNumData.csv')\n",
    "\n",
    "# import the categorical data\n",
    "data = pd.read_csv('data/AmesHousingCatData.csv')\n",
    "\n",
    "# 'index=False' as parameter for exporting the files is a better solution\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256dbb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into input and target\n",
    "target = data['SalePrice']\n",
    "# final_data = data[['Gr Liv Area', 'Overall Qual']]\n",
    "final_data = data.drop('SalePrice', axis=1)\n",
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dac13f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the pandas dataframes to numpy ndarrays\n",
    "X_np = final_data.to_numpy()\n",
    "y_np = target.to_numpy()\n",
    "\n",
    "# split the data into 70% training and 30% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_np, y_np, train_size=0.7, random_state=1265599650)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affdb2dd",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eda4147",
   "metadata": {},
   "source": [
    "#### Visualizing a single feature "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a9914e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make column vector out of row vector\n",
    "y_train = y_train[..., None]\n",
    "\n",
    "# make column vector for one feature for testing\n",
    "X_single_feature = X_train[:, 1, None]\n",
    "\n",
    "# retrieve name of said feature\n",
    "feature_name = final_data.columns.values[1]\n",
    "\n",
    "# plot the datapoints\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(X_single_feature, y_train, 'r.')\n",
    "plt.title(f'Relation of {feature_name} and Sale Price')\n",
    "plt.xlabel(f'{feature_name}')\n",
    "plt.ylabel('Sale Price normalized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae8a2b8",
   "metadata": {},
   "source": [
    "#### Adding  $x_0$to the data\n",
    "\n",
    "First an extra feature $x_0$ has to be added to the data. This is to make the computations easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513ac42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_x0(x):\n",
    "    '''\n",
    "    This function takes a column vector x, adds a column vector of ones in front of it, and returns the \n",
    "    combined matrix.\n",
    "    '''\n",
    "    # create new feature vector of ones in shape of number of samples * 1 column vector \n",
    "    x_zero = np.ones((len(x), 1), dtype = int)\n",
    "\n",
    "    # combine the new and old feature vectors into matrix\n",
    "    return np.hstack((x_zero, x))\n",
    "\n",
    "# create feature with x0\n",
    "X = add_x0(X_single_feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78657e01",
   "metadata": {},
   "source": [
    "#### Defining the linear model\n",
    "\n",
    "\n",
    "First, we implement **univariate linear regression** with the feature selected above. The system of linear equations looks like this:\n",
    "\n",
    "$$x^1_0 \\theta_0 + x^1_1 \\theta_1 = h_{\\theta}(x^1)$$\n",
    "$$x^2_0 \\theta_0 + x^2_1 \\theta_1 = h_{\\theta}(x^2)$$\n",
    "$$x^3_0 \\theta_0 + x^3_1 \\theta_1 = h_{\\theta}(x^3)$$ \n",
    "$$\\dots$$\n",
    "$$x^m_0 \\theta_0 + x^m_1 \\theta_1 = h_{\\theta}(x^m)$$\n",
    "\n",
    "\n",
    "x_0 is the column vector of ones that were added in front of the column vector of the selected feature. \n",
    "\n",
    "In terms of matrix multiplication:\n",
    "\n",
    "$$ \\left[\\begin{array}{cccc}\n",
    "x^1_0 & x^1_1 \\\\ \n",
    "x^2_0 & x^2_1 \\\\\n",
    "x^3_0 & x^3_1 \\\\ \n",
    "\\vdots & \\vdots \\\\\n",
    "x^m_0 & x^m_1 \\\\ \n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\end{array} \\right]\n",
    "= \\left[\\begin{array}{c} h_{\\theta}(x^1) \\\\ h_{\\theta}(x^2) \\\\\n",
    "h_{\\theta}(x^3) \\\\ \\vdots \\\\ h_{\\theta}(x^m) \\end{array} \\right]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8a1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_theta(X):\n",
    "    '''\n",
    "    Returns starting vector for theta with amount of features.\n",
    "    '''\n",
    "    return np.ones((X.shape[1], 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c83a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X, theta):\n",
    "    '''\n",
    "    This function takes a matrix X and a vector theta and returns the resulting column vector of hypothesis\n",
    "    values.\n",
    "    '''\n",
    "    return np.matmul(X, theta)\n",
    "\n",
    "theta = define_theta(X)\n",
    "\n",
    "# compute the linear model\n",
    "h = linear_model(X, theta)\n",
    "\n",
    "# plot the model results for just one of the features\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(X_single_feature, y_train, 'r.')\n",
    "plt.plot(X_single_feature, h)\n",
    "plt.title(f'Relation of {feature_name} and Sale Price')\n",
    "plt.xlabel(f'{feature_name}')\n",
    "plt.ylabel('Sale Price normalized')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cb83d6",
   "metadata": {},
   "source": [
    "### Optimizing theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd4029",
   "metadata": {},
   "source": [
    "#### Defining linear cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1face454",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_cost(theta, X, y):\n",
    "    '''\n",
    "    This function returns the cost using matrix multiplication. \n",
    "    '''\n",
    "    # m is identical to the number of rows in X\n",
    "    m = X.shape[0]\n",
    "\n",
    "\n",
    "    # compute error vector \n",
    "    residual = linear_model(X, theta) - y\n",
    "    \n",
    "    # compute cost vector  \n",
    "    cost = np.matmul(residual.T, residual) / (2 * m)\n",
    "    \n",
    "    # convert the resulting 1 x 1 numpy matrix to an actual scalar \n",
    "    return np.squeeze(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b079871",
   "metadata": {},
   "source": [
    "#### Defining gradient vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648d30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_vector(theta, X, y):\n",
    "    '''\n",
    "    This functions returns a (n + 1) * 1 column vector of partial derivatives. \n",
    "    '''\n",
    "    # m is identical to the number of rows in X\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # compute error vector\n",
    "    error = linear_model(X, theta) - y \n",
    "    \n",
    "    # compute matrix product of the error and X\n",
    "    gradient_vector = np.matmul(X.T, error) / m\n",
    "  \n",
    "    return gradient_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6f7e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, theta, alpha, thres=10**-6):\n",
    "    '''\n",
    "    This function returns a vector for theta for which the cost is the minimum. \n",
    "    '''\n",
    "    # calculate cost with current theta\n",
    "    previous_cost = linear_cost(theta, X, y)\n",
    "    \n",
    "    # set cost difference to amount bigger than the threshold and bigger than 0\n",
    "    cost_difference = thres + 1\n",
    "    \n",
    "    # loop indefinitely until algorithm converges\n",
    "    while cost_difference > thres: \n",
    "        \n",
    "        # compute and update theta parameters in a single operation\n",
    "        theta = theta - alpha * gradient_vector(theta, X, y)\n",
    "\n",
    "        # calculate cost vector with new theta\n",
    "        current_cost = linear_cost(theta, X, y)\n",
    "\n",
    "        cost_difference = abs(current_cost - previous_cost)\n",
    "        \n",
    "        #print(cost_difference)\n",
    "\n",
    "        if current_cost > previous_cost:\n",
    "            print('The algorithm is diverging.')\n",
    "            return theta, previous_cost\n",
    "\n",
    "        previous_cost = current_cost\n",
    "    \n",
    "    return theta, previous_cost\n",
    "\n",
    "# find the theta vector that minimizes the cost function\n",
    "theta_hat, cost = gradient_descent(X, y_train, theta, 0.9)\n",
    "print(\"\\nFound the following values for theta:\")\n",
    "print(theta_hat)\n",
    "print(f'\\nThe cost is {cost}.')\n",
    "\n",
    "# plot the model results\n",
    "print(\"\\nResulting in the following model:\")\n",
    "h = linear_model(X, theta_hat)\n",
    "plt.figure(figsize=(12,9))\n",
    "plt.plot(X_single_feature, y_train, 'r.')\n",
    "plt.plot(X_single_feature, h)\n",
    "plt.title(f'Relation of {feature_name} and Sale Price')\n",
    "plt.xlabel(f'{feature_name}')\n",
    "plt.ylabel('Sale Price in logarithmic scale')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c060d5f6",
   "metadata": {},
   "source": [
    "#### Visualizing multiple features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab45a043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# make matrix for all features (two for now)\n",
    "X_all_train = X_train[:, 0:]\n",
    "\n",
    "#retrieve name of second feature\n",
    "second_feature_name = final_data.columns.values[0]\n",
    "\n",
    "#create 3d plot\n",
    "ax = Axes3D(plt.figure(figsize=(12,9)))\n",
    "\n",
    "# retrieve datapoints and plot\n",
    "xs = X_all_train[:, 0]\n",
    "ys = X_all_train[:, 1]\n",
    "zs = y_train\n",
    "ax.scatter(xs, ys, zs)\n",
    "\n",
    "# set layout\n",
    "ax.set_xlabel(f'{second_feature_name}', fontsize=14)\n",
    "ax.set_ylabel(f'{feature_name}', fontsize=14)\n",
    "ax.set_zlabel('Sale Price in logarithmic scale', fontsize=14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a211ec91",
   "metadata": {},
   "source": [
    "#### Defining multivariate linear regression\n",
    "\n",
    "Our model is a **multivariate linear** regression model which will look like the following:\n",
    "\n",
    "$$x^1_0 \\theta_0 + x^1_1 \\theta_1 + \\dots + x^1_n \\theta_n = h_{\\theta}(x^1)$$\n",
    "$$x^2_0 \\theta_0 + x^2_1 \\theta_1 + \\dots + x^2_n \\theta_n = h_{\\theta}(x^2)$$\n",
    "$$x^2_0 \\theta_0 + x^2_1 \\theta_1 + \\dots + x^2_n \\theta_n = h_{\\theta}(x^2)$$\n",
    "$$\\dots$$\n",
    "$$x^m_0 \\theta_0 + x^m_1 \\theta_1 + \\dots + x^m_n \\theta_n = h_{\\theta}(x^m)$$\n",
    "\n",
    "\n",
    "In terms of matrix multiplication:\n",
    "\n",
    "$$ \\left[\\begin{array}{cccc}\n",
    "x_0^1 & x_1^1 & x_2^1 & \\cdots & x_n^1 \\\\ \n",
    "x_0^2 & x_1^2 & x_2^2 & \\cdots & x_n^2 \\\\ \n",
    "x_0^3 & x_1^3 & x_2^3 & \\cdots & x_n^3 \\\\ \n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "x_0^m & x_1^m & x_2^m & \\cdots & x_n^m \\\\ \n",
    "\\end{array} \\right]\n",
    "\\left[\\begin{array}{c} \\theta_0 \\\\ \\theta_1 \\\\\n",
    "\\theta_2 \\\\ \\vdots \\\\ \\theta_n \\end{array} \\right]\n",
    "= \\left[\\begin{array}{c} h_{\\theta}(x^1) \\\\ h_{\\theta}(x^2) \\\\\n",
    "h_{\\theta}(x^3) \\\\ \\vdots \\\\ h_{\\theta}(x^m) \\end{array} \\right]$$\n",
    "\n",
    "With *n* being the number of features and *m* the number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8cba8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add x zero\n",
    "X_train_x0 = add_x0(X_all_train)\n",
    "\n",
    "theta = define_theta(X_train_x0)\n",
    "\n",
    "# compute the linear model\n",
    "h = linear_model(X_train_x0, theta)\n",
    "\n",
    "theta_hat, cost = gradient_descent(X_train_x0, y_train, theta, 0.05)\n",
    "print(\"\\nFound the following values for theta:\")\n",
    "print(theta_hat)\n",
    "print(f'\\nThe cost is {cost}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeeec5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_learning_curves(X_train, y_train, X_test, y_test, theta, alpha, thres):\n",
    "    '''\n",
    "    Implements gradient descent, stores training and testing cost at every step, and plots learning curves.\n",
    "    '''\n",
    "    # initialize number of iterations\n",
    "    iteration = 0\n",
    "    iterations = []\n",
    "    \n",
    "    # create lists for training and testing cost to keep track at each step\n",
    "    current_costs_train = []\n",
    "    current_costs_test = []\n",
    "\n",
    "    # compute training cost with start theta\n",
    "    previous_cost = linear_cost(theta, X_train, y_train)\n",
    "\n",
    "    cost_diff = thres + 1\n",
    "\n",
    "    while cost_diff > thres:\n",
    "\n",
    "        iteration += 1\n",
    "        iterations.append(iteration)\n",
    "        \n",
    "        # compute new theta\n",
    "        theta = theta - alpha * gradient_vector(theta, X_train, y_train)\n",
    "        \n",
    "        # compute new training cost and append it to a list\n",
    "        current_cost_train = linear_cost(theta, X_train, y_train)\n",
    "        current_costs_train.append(current_cost_train)\n",
    "        \n",
    "        # compute new testing cost and append it to a list\n",
    "        current_cost_test = linear_cost(theta, X_test, y_test)\n",
    "        current_costs_test.append(current_cost_test)\n",
    "\n",
    "        # compare the old cost vs the new cost\n",
    "        cost_diff = previous_cost - current_cost_train\n",
    "\n",
    "        # check if the algorithm is diverging\n",
    "        if cost_diff < 0:\n",
    "            print('The algorithm is diverging.')\n",
    "\n",
    "        previous_cost = current_cost_train\n",
    "        \n",
    "    #print(current_costs_train)\n",
    "    # plot learning curves\n",
    "    plt.plot(iterations, current_costs_train, 'b-', label='Training')\n",
    "    plt.plot(iterations, current_costs_test, 'r-', label='Testing')    \n",
    "    plt.xlabel('Number of iterations')\n",
    "    plt.ylabel('Cost')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return theta, current_cost_train, current_cost_test\n",
    "\n",
    "# make matrix for all features \n",
    "X_all_test = X_test[:, 0:]\n",
    "\n",
    "# add x zero\n",
    "X_test_x0 = add_x0(X_all_test)\n",
    "\n",
    "# make column vector out of row vector\n",
    "y_test = y_test[..., None]\n",
    "\n",
    "# Initialize theta to a zero vector of the correct shape\n",
    "theta = define_theta(X_train_x0)\n",
    "\n",
    "#Find the theta vector that minimizes the cost functions\n",
    "plt.title('Learning curves of gradient descent with learning rate = 0.2, threshold = 10**-6\\n')\n",
    "theta_hat, cost_train, cost_test = gradient_descent_learning_curves(X_train_x0, y_train, X_test_x0, y_test, theta, 0.2, 10**-6)\n",
    "print(f'\\nThe training cost is {cost_train}. The testing cost is {cost_test}.')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1664b8e",
   "metadata": {},
   "source": [
    "#### Conclusion\n",
    "\n",
    "*We tested three different values for the learning rate and the convergence threshold of gradient descent. We first used learning rate 10-4 and threshold 10**-6. Since this is a rather small value for the learning rate, it takes more than 20000 steps until the algorithm converges. This makes the algorithm rather slow. We then tested two slightly larger values for both, namely learning rate 10-3, threshold 10-4. Since the learning rate is bigger, the algorithm needs less steps, namely around 1200 to converge. Since the threshold is larger, the algorithm converges at a higher cost. Therefore, we made the learning rate even higher, but changed the threshold back to the one I used before. The cost is lower than either cost before, and the algorithm needs less steps to converge. The final model is therefore the best one so far.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "073e6d96",
   "metadata": {},
   "source": [
    "#### Prediction accuracy\n",
    "\n",
    "Taken from https://www.kaggle.com/kamalp0/ames-housing-regression-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2460cf62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "predictions = linear_model(X_test_x0, theta_hat)\n",
    "\n",
    "# calculate average error\n",
    "MSE = mean_squared_error(y_test, predictions)\n",
    "\n",
    "# calculate standard deviation\n",
    "RMSE = np.sqrt(MSE)\n",
    "\n",
    "# calculate average price\n",
    "avg_price = np.mean(target)\n",
    "\n",
    "off_by = (RMSE)/ avg_price* 100\n",
    "\n",
    "print(f'The theta values for our linear model are:\\n{theta_hat}')\n",
    "print(f'The test voor MSE for our linear model was {MSE}')\n",
    "print(f'The test RMSE for our linear model {RMSE}')\n",
    "print(f'The percentage our model was off compared to the real house price was:{off_by}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98ea087",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "linear_model2 = LinearRegression()\n",
    "\n",
    "linear_model2.fit(X_train, y_train)\n",
    "\n",
    "predict = linear_model2.predict(X_test)\n",
    "\n",
    "linear_test_mae = mean_absolute_error(y_test, predict)\n",
    "linear_test_rmse = np.sqrt(mean_squared_error(y_test, predict))\n",
    "\n",
    "print(f'The test MAE for our linear model {linear_test_mae}')\n",
    "print(f'The test RMSE for our linear model {linear_test_rmse}')\n",
    "\n",
    "print(f'The theta values for the sklearn linear model are:\\n{linear_model2.intercept_} and {linear_model2.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e8fbf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
